model_train.py HyperBand Search
Best Hyperparameters: 
{'num_LSTM_layers': 1, 
'units_first': 32, 
'activation': 'relu', 
'recurrent_activation': 'silu', 
'kernel_initializer': 'glorot_uniform', 
'recurrent_initializer': 'he_normal', 
'bias_initializer': 'glorot_normal', 
'forget_bias': True, 
'dropout': 0.35000000000000003, 
'recurrent_dropout': 0.2, 
'num_dense_layers': 2, 
'units': 32, 
'learning_rate': 0.0035708917935944437, 
'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 4, 'tuner/round': 0}

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 1, 32)             36096     
                                                                 
 lstm_1 (LSTM)               (None, 32)                8320      
                                                                 
 dense (Dense)               (None, 32)                1056      
                                                                 
 dense_1 (Dense)             (None, 32)                1056      
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 46561 (181.88 KB)
Trainable params: 46561 (181.88 KB)
Non-trainable params: 0 (0.00 Byte)

Bayesian Optimization
Best Hyperparameters: 
{'num_LSTM_layers': 2, 
'units_first': 32, 
'activation': 'relu', 
'recurrent_activation': 'elu', 
'kernel_initializer': 'glorot_uniform', 
'recurrent_initializer': 'glorot_normal', 
'bias_initializer': 'glorot_uniform', 
'forget_bias': True, 
'dropout': 0.30000000000000004, 
'recurrent_dropout': 0.30000000000000004, 
'num_dense_layers': 2, 
'units': 64}
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 1, 32)             36096     
                                                                 
 lstm_1 (LSTM)               (None, 1, 32)             8320      
                                                                 
 lstm_2 (LSTM)               (None, 32)                8320      
                                                                 
 dense (Dense)               (None, 64)                2112      
                                                                 
 dense_1 (Dense)             (None, 64)                4160      
                                                                 
 dense_2 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 59073 (230.75 KB)
Trainable params: 59073 (230.75 KB)
Non-trainable params: 0 (0.00 Byte)

ML_TEST.py
vix_close w5_8_13_signal mfi_signal  ... kama_signal PPO_signal Ao_signal
0           NaN              0          0  ...         0.0          0         0
1     12.330000              0          0  ...         0.0          0         0
2     12.250000              0          0  ...         0.0          0         0
3     12.120000              0          0  ...         0.0          0         0
4     12.290000              0          0  ...         0.0          0         0
...         ...            ...        ...  ...         ...        ...       ...
7911        NaN            NaN        NaN  ...         NaN        NaN       NaN
7912  17.459999            NaN        NaN  ...         NaN        NaN       NaN
7913  17.940001            NaN        NaN  ...         NaN        NaN       NaN
7914  15.650000            NaN        NaN  ...         NaN        NaN       NaN
7915  14.600000            NaN        NaN  ...         NaN        NaN       NaN
[7916 rows x 38 columns]

Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',
       'volume_adi', 'volume_obv', 'volume_cmf', 'volume_fi', 'volume_em',
       'volume_sma_em', 'volume_vpt', 'volume_vwap', 'volume_mfi',
       'volume_nvi', 'volatility_bbm', 'volatility_bbh', 'volatility_bbl',
       'volatility_bbw', 'volatility_bbp', 'volatility_bbhi',
       'volatility_bbli', 'volatility_kcc', 'volatility_kch', 'volatility_kcl',
       'volatility_kcw', 'volatility_kcp', 'volatility_kchi',
       'volatility_kcli', 'volatility_dcl', 'volatility_dch', 'volatility_dcm',
       'volatility_dcw', 'volatility_dcp', 'volatility_atr', 'volatility_ui',
       'trend_macd', 'trend_macd_signal', 'trend_macd_diff', 'trend_sma_fast',
       'trend_sma_slow', 'trend_ema_fast', 'trend_ema_slow',
       'trend_vortex_ind_pos', 'trend_vortex_ind_neg', 'trend_vortex_ind_diff',
       'trend_trix', 'trend_mass_index', 'trend_dpo', 'trend_kst',
       'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_conv',
       'trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b',
       'trend_stc', 'trend_adx', 'trend_adx_pos', 'trend_adx_neg', 'trend_cci',
       'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_up',
       'trend_aroon_down', 'trend_aroon_ind', 'trend_psar_up',
       'trend_psar_down', 'trend_psar_up_indicator',
       'trend_psar_down_indicator', 'momentum_rsi', 'momentum_stoch_rsi',
       'momentum_stoch_rsi_k', 'momentum_stoch_rsi_d', 'momentum_tsi',
       'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr',
       'momentum_ao', 'momentum_roc', 'momentum_ppo', 'momentum_ppo_signal',
       'momentum_ppo_hist', 'momentum_pvo', 'momentum_pvo_signal',
       'momentum_pvo_hist', 'momentum_kama', 'others_dr', 'others_dlr',
       'others_cr'],
      dtype='object')

        Date      Open      High  ...  signals_0  signals_long  signals_short
0        0.0  43.96875  43.96875  ...      False          True          False
1        1.0  43.96875  44.25000  ...       True         False          False
2        2.0  44.21875  44.37500  ...       True         False          False
3        3.0  44.40625  44.84375  ...       True         False          False
4        4.0  44.96875  45.09375  ...       True         False          False
...      ...       ...       ...  ...        ...           ...            ...
7911  7911.0   0.00000   0.00000  ...       True         False          False
7912  7912.0   0.00000   0.00000  ...       True         False          False
7913  7913.0   0.00000   0.00000  ...       True         False          False
7914  7914.0   0.00000   0.00000  ...       True         False          False
7915  7915.0   0.00000   0.00000  ...       True         False          False
[7916 rows x 227 columns]

LONG Signals Machine Learning Prediction
      Regression Model  Explained Variance Score  Mean Absolute Error
0                  LiR             -9.910084e+09           362.658513
1                Ridge              5.850395e-01             0.040735
2               Bag_Re              9.615643e-01             0.001136
3         RandomForest              9.701972e-01             0.001016
4  ExtraTreesRegressor              9.762703e-01             0.001225
5                  KNN              8.567514e-02             0.029167
6                 CART              9.699620e-01             0.000631
7           NNwDropout              1.998735e-08             0.021465
8          NNwoDropout              1.998735e-08             0.021465

SHORT Signals Machine Learning Prediciton
      Regression Model  Explained Variance Score  Mean Absolute Error
0                  LiR             -1.394929e+10           362.658513
1                Ridge              4.155791e-01             0.040717
2               Bag_Re              9.497310e-01             0.001073
3         RandomForest              9.595596e-01             0.000915
4  ExtraTreesRegressor              9.695954e-01             0.001275
5                  KNN             -4.972650e-02             0.022222
6                 CART              9.154915e-01             0.001263
7           NNwDropout             -2.302803e-07             0.015152
8          NNwoDropout             -2.302803e-07             0.015152

Here, it seems that the "RandomForest" and "ExtraTreesRegressor" models perform best for LONG signals, 
as they have the highest Explained Variance Score and the lowest Mean Absolute Error. Other models, like "KNN" and "LiR," perform less effectively.

Similar to the LONG signals section, this section provides evaluation metrics for SHORT signals, and again, 
"RandomForest" and "ExtraTreesRegressor" appear to be the best-performing models based on Explained Variance Score and Mean Absolute Error.


trend_trial.py

Random Forest
Best Hyperparameter: {'n_estimators': 140}
Classification Report
              precision    recall  f1-score   support

       False       0.99      1.00      1.00      1550
        True       1.00      0.65      0.79        34

    accuracy                           0.99      1584
   macro avg       1.00      0.82      0.89      1584
weighted avg       0.99      0.99      0.99      1584
Confusion Matrix
[[1550    0]
 [  12   22]]

KNN
Best Hyperparameter: {'n_neighbors': 3}
Classification Report
              precision    recall  f1-score   support

       False       0.98      1.00      0.99      1550
        True       0.38      0.09      0.14        34

    accuracy                           0.98      1584
   macro avg       0.68      0.54      0.57      1584
weighted avg       0.97      0.98      0.97      1584
Confusion Matrix
[[1545    5]
 [  31    3]]

Ensemble
Ensemble Score: 0.9804292929292929
Classification Report
              precision    recall  f1-score   support

       False       0.98      1.00      0.99      1550
        True       1.00      0.09      0.16        34

    accuracy                           0.98      1584
   macro avg       0.99      0.54      0.58      1584
weighted avg       0.98      0.98      0.97      1584
Confusion Matrix
[1550    0]
[  31    3]

A high precision indicates a low false positive rate, while a high recall indicates a low false negative rate. The F1-score is the harmonic mean of precision and recall.
Confusion Matrix: This matrix shows the number of true positives, true negatives, false positives, and false negatives for each model. 
For example, in the Random Forest model, it correctly predicted 1550 "False" values and 22 "True" values. However, it incorrectly predicted 12 "True" values as "False."
In these classification models, "Random Forest" has high accuracy and performs better than "KNN." The "Ensemble" section indicates that the ensemble 
model combines these two models and also performs well.

train.py:

Linear Regression Stats:
Mean Squared Error: 1.2187524660680467
Explained Variance Score: 0.9998874576295881
R^2 Score: 0.999887386535229

Ridge Regression Stats:
Mean Squared Error: 0.9746393399248
Explained Variance Score: 0.9999099609229404
R^2 Score: 0.9999099427356852
Best Parameters: {'alpha': 0.1, 'solver': 'svd'}
Best Ridge Regression Model: Ridge(alpha=0.1, max_iter=10000, solver='svd')

Decision Tree Stats:
Mean Squared Error: 0.0875507322157879
Explained Variance Score: 0.9999919132092684
R^2 Score: 0.9999919102594066
Best Parameters: {'max_depth': 40, 'min_samples_leaf': 4, 'min_samples_split': 2}
Best Model: DecisionTreeRegressor(max_depth=40, min_samples_leaf=4)

Random Forest Stats:
Mean Squared Error: 0.05583113407796384
Explained Variance Score: 0.9999948431791869
R^2 Score: 0.9999948411694535
Best Parameters: {'max_depth': 10, 'n_estimators': 100}
Best Model: RandomForestRegressor(max_depth=10)

Lasso Stats:
Mean Squared Error: 1.2068376878071454
R^2 Score: 0.9998884874679445
Best Parameters: {'alpha': 0.01, 'tol': 1e-06}
Best Model: Lasso(alpha=0.01, max_iter=100000, tol=1e-06)

LSTM Stats:
Mean Squared Error: 36403.04451033533
R^2 Score: -767609.9627440673
Best Parameters: {'batch_size': 32, 'optimizer': 'adam'}
Best Model: KerasRegressor(
        model=<function create_lstm_model at 0x0000020B9737A670>
        build_fn=None
        warm_start=False
        random_state=None
        optimizer=adam
        loss=None
        metrics=None
        batch_size=32
        validation_batch_size=None
        verbose=0
        callbacks=None
        validation_split=0.0
        shuffle=True
        run_eagerly=False
        epochs=10
)

DNN Stats:
Mean Squared Error: 1.2461859201746874
R^2 Score: 0.9998848516674822
Best Parameters: {'batch_size': 16, 'epochs': 30, 'units': 32}
Best Model: KerasRegressor(
        model=<function create_dnn_model at 0x00000278405795E0>
        build_fn=None
        warm_start=False
        random_state=None
        optimizer=rmsprop
        loss=None
        metrics=None
        batch_size=16
        validation_batch_size=None
        verbose=0
        callbacks=None
        validation_split=0.0
        shuffle=True
        run_eagerly=False
        epochs=30
        units=32
)

RNN Stats:
Mean Squared Error: 35114.431171161144
R^2 Score: -740437.6825352648
Best Parameters: {'units': 50, 'optimizer': 'sgd'}
Best Model: KerasRegressor(
        model=None
        build_fn=<function create_rnn_model at 0x00000191D43E75E0>
        warm_start=False
        random_state=None
        optimizer=sgd
        loss=None
        metrics=None
        batch_size=32
        validation_batch_size=None
        verbose=0
        callbacks=None
        validation_split=0.0
        shuffle=True
        run_eagerly=False
        epochs=10
        units=50
)

regression_training.py
Ridge Stats:
Mean Squared Error: 0.9746393399248
Explained Variance Score: 0.9999099609229404
R^2 Score: 0.9999099427356852
Best Parameters: {'alpha': 0.1, 'solver': 'svd'}
Best Model: Ridge(alpha=0.1, max_iter=10000, solver='svd')

Lasso Stats:
Mean Squared Error: 1.2068376878071454
Explained Variance Score: 0.9998885562032119
R^2 Score: 0.9998884874679445
Best Parameters: {'alpha': 0.01, 'tol': 1e-06}
Best Model: Lasso(alpha=0.01, max_iter=100000, tol=1e-06)

DecisionTreeRegressor Stats:
Mean Squared Error: 0.13324700806069636
Explained Variance Score: 0.9999877402872241
R^2 Score: 0.9999876878958888
Best Parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5}
Best Model: DecisionTreeRegressor(min_samples_leaf=4, min_samples_split=5)

RandomForestRegressor Stats:
Mean Squared Error: 0.06927601484394325
Explained Variance Score: 0.9999936053186438
R^2 Score: 0.9999935988543414
Best Parameters: {'max_depth': None, 'n_estimators': 100}
Best Model: RandomForestRegressor()

CCA Stats:
Mean Squared Error: 5.253466568775266
Explained Variance Score: 0.9995146937239838
R^2 Score: 0.9995145765125901
Best Parameters: {'n_components': 1}
Best Model: CCA(n_components=1)

PLSCanonical Stats:
Mean Squared Error: 245410.77512474757
Explained Variance Score: -21.674450122377916
R^2 Score: -21.67610400665294
Best Parameters: {'n_components': 1}
Best Model: PLSCanonical(n_components=1)

PLSRegression Stats:
Mean Squared Error: 334.1961731150638
Explained Variance Score: 0.9691351452082279
R^2 Score: 0.969120071535855
Best Parameters: {'n_components': 1}
Best Model: PLSRegression(n_components=1)

DummyRegressor Stats:
Mean Squared Error: 10822.591440505772
Explained Variance Score: 0.0
R^2 Score: -1.3992872445856008e-05
Best Parameters: {'constant': None, 'quantile': None, 'strategy': 'mean'}
Best Model: DummyRegressor()

AdaBoostRegressor Stats:
Mean Squared Error: 109.94963241475666
Explained Variance Score: 0.9898707286650577
R^2 Score: 0.9898405874849509
Best Parameters: {'learning_rate': 1.0, 'n_estimators': 200}
Best Model: AdaBoostRegressor(n_estimators=200)

BaggingRegressor Stats:
Mean Squared Error: 0.06700909621461838
Explained Variance Score: 0.999993813426679
R^2 Score: 0.9999938083189934
Best Parameters: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 20}
Best Model: BaggingRegressor(n_estimators=20)

ExtraTreesRegressor Stats:
Mean Squared Error: 0.15249990432727398
Explained Variance Score: 0.9999859131612975
R^2 Score: 0.9999859089166326
Best Parameters: {'max_depth': 20, 'n_estimators': 200}
Best Model: ExtraTreesRegressor(max_depth=20, n_estimators=200)

lstm_train.py
Model 1 - RMSE: 0.003, MAE: 0.002
Model 2 - RMSE: 0.004, MAE: 0.003
Model 3 - RMSE: 0.176, MAE: 0.127
Model 4 - RMSE: 0.002, MAE: 0.001


classification_training.py

